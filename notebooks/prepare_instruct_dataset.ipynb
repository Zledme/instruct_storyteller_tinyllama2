{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3969128d-2e3c-4d20-8d34-18c5bdab4bc5",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02281434-f8a1-40a4-acda-aff972804ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db571644-96f2-4e45-9b38-c525cce17b2b",
   "metadata": {},
   "source": [
    "# paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eab4d98-452e-46d1-9ca4-2e1a1d96a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '../data/TinyStories_all_data/'\n",
    "data = '/home/cindy/learning/karpathy/llms/llama2.c/data/TinyStories_all_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acebf20-8fba-4a9b-858b-126632c1f725",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = '../tokenizer.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad60a4d7-1a54-4a72-b0cd-025faaafca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_data = '../instruct_dataset/'\n",
    "os.makedirs(out_data, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92b9e6e-ac2b-42b3-a7b3-92fcd2cad762",
   "metadata": {},
   "source": [
    "# preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcf7e59e-0e50-4988-9ac2-61564298fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_words(\n",
    "    prompt, \n",
    "    words_list\n",
    "):\n",
    "    for words_to_remove, replacement in words_list:\n",
    "        prompt = prompt.replace(words_to_remove, replacement)\n",
    "    return prompt\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e183c45-1005-46c6-bf55-ae2279053a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657b224b-7e6c-4034-a6c9-d4cbd7400ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_prompt(prompt, suffix=' Possible story:'):\n",
    "    words_list = [\n",
    "    (\"Write a short story (3-5 paragraphs) which only uses very simple words that a 3 year old child would understand.\", \"Write a story.\"),\n",
    "    (\" Remember to only use simple words!\", \"\"),\n",
    "    (\"\\n\\nPossible story:\", \"\"),\n",
    "    (\"try to at some point use\", \"try to use\")\n",
    "    ]\n",
    "    prompt = replace_words(prompt, words_list)\n",
    "    return prompt + suffix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b3d9b4-0113-4c2b-8ab2-639270d4aa08",
   "metadata": {},
   "source": [
    "# tokenize and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c766581-4d9e-4bb5-b9e9-1ca15fb40003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(out_folder, filename, data_to_save):\n",
    "    filepath = os.path.join(out_folder, filename)\n",
    "    print(out_folder, filename, data_to_save.shape)\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        f.write(data_to_save.tobytes())\n",
    "    print('Saved to ', filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3bcaec-59dc-4131-8bcd-68cd302cb0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_chunk(chunk_path, out_folder, tokenizer, max_seq_len, pad_token):\n",
    "    with open(chunk_path, 'r') as f:\n",
    "        chunk = json.load(f)\n",
    "        print('Tokenize ', chunk_path)\n",
    "    all_tokens = []\n",
    "    all_labels = []\n",
    "    for sample in tqdm(chunk):\n",
    "        story = sample['story'].strip()\n",
    "        prompt = preprocess_prompt(sample['instruction']['prompt:'].strip())\n",
    "        tokenized_prompt = tokenizer.encode(prompt)\n",
    "        \n",
    "        prompt_and_story = tokenized_prompt + [tokenizer.bos_id()] + tokenizer.encode(story) + [tokenizer.eos_id()]\n",
    "        label = [pad_token]*len(tokenized_prompt) + [tokenizer.bos_id()] + tokenizer.encode(story) + [tokenizer.eos_id()]\n",
    "\n",
    "        if len(prompt_and_story) <= max_seq_len:\n",
    "            prompt_and_story += [pad_token] * (max_seq_len - len(prompt_and_story))\n",
    "            label += [pad_token] * (max_seq_len - len(label))\n",
    "            assert len(prompt_and_story) == len(label) == max_seq_len\n",
    "            all_tokens.extend(prompt_and_story)\n",
    "            all_labels.extend(label)\n",
    "\n",
    "    all_tokens = np.array(all_tokens, dtype=np.int16)\n",
    "    all_labels = np.array(all_labels, dtype=np.int16)\n",
    "    \n",
    "    all_tokens_filename = chunk_path.split('/')[-1].replace('.json', '.bin')\n",
    "    save(out_folder=out_folder, filename=all_tokens_filename, data_to_save=all_tokens)\n",
    "\n",
    "    all_labels_filename = all_tokens_filename.replace('data', 'labels')\n",
    "    save(out_folder=out_folder, filename=all_labels_filename, data_to_save=all_labels)\n",
    "\n",
    "        \n",
    "    \n",
    "def tokenize_all_chunks(data, out_folder, tokenizer, max_seq_len, pad_token, max_workers=5):\n",
    "    tokenize_chunk_fn = partial(\n",
    "        tokenize_chunk,\n",
    "        out_folder=out_folder,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_len=max_seq_len,\n",
    "        pad_token=pad_token\n",
    "    )\n",
    "        \n",
    "    tokenize_chunk_paths = [os.path.join(data, fn) for fn in os.listdir(data) if fn.endswith('.json')]\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        executor.map(tokenize_chunk_fn, tokenize_chunk_paths)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f6b093-e970-45fd-8ee4-094ae00c9ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e1f27-c067-49ca-82bb-222cd92a676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SentencePieceProcessor(model_file=tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a15a078-c8e7-4fa2-9dca-48e99461e819",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_all_chunks(\n",
    "    data=data, \n",
    "    out_folder=out_data, \n",
    "    tokenizer=tokenizer, \n",
    "    max_seq_len=350,\n",
    "    pad_token=-100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79269035-875b-4c8d-95c1-fc3fb4eac912",
   "metadata": {},
   "source": [
    "# check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8d5834-8255-4345-97c8-ac5a4cb0f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(out_data + 'data11.bin', 'rb') as f:\n",
    "    x = f.read()\n",
    "x = np.frombuffer(x, dtype=np.int16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919858be-5a9d-45cf-91e5-05aa244f35a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(out_data + 'labels11.bin', 'rb') as f:\n",
    "    y = f.read()\n",
    "y = np.frombuffer(y, dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904c9c18-ed45-423f-87a1-2293c800ae3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llama2karpathy]",
   "language": "python",
   "name": "conda-env-llama2karpathy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
