{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from contextlib import nullcontext\n",
    "import matplotlib.pyplot as plt\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "from data_loader import *\n",
    "from utils import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "device_type = 'cuda'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_CACHE_DIR = '../instruct_dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '../models/'\n",
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SentencePieceProcessor('../tokenizer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mixed precision settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = 'bfloat16'\n",
    "torch.manual_seed(1337)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "device_type = \"cuda\" if \"cuda\" in device else \"cpu\" \n",
    "ptdtype = {\"float32\": torch.float32, \"bfloat16\": torch.bfloat16, \"float16\": torch.float16}[dtype]\n",
    "\n",
    "ctx = (\n",
    "    nullcontext()\n",
    "    if device_type == \"cpu\"\n",
    "    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == \"float16\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 288 #288 #\n",
    "n_layers =  6\n",
    "n_heads =  6\n",
    "n_kv_heads = n_heads\n",
    "multiple_of = 32\n",
    "dropout = 0.0\n",
    "max_seq_len = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ModelArgs(\n",
    "    dim=dim,\n",
    "    n_layers=n_layers,\n",
    "    n_heads=n_heads,\n",
    "    n_kv_heads=n_heads,\n",
    "    vocab_size=32000,\n",
    "    multiple_of=multiple_of,\n",
    "    max_seq_len=max_seq_len,\n",
    "    dropout=dropout,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(model_args)\n",
    "model.to(device);\n",
    "print(f'Number of parameters: {sum(p.nelement() for p in model.parameters())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "wanted_batch_size = 4 * 128\n",
    "gradient_accumulation_steps = wanted_batch_size // batch_size\n",
    "\n",
    "print(f'Wanted batch_size: {wanted_batch_size}, gradient accumulation steps: {gradient_accumulation_steps}, batch_size: {batch_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_batches = partial(\n",
    "    iter_batch_func,\n",
    "    device=device,\n",
    "    batch_size=batch_size,\n",
    "    max_seq_len=max_seq_len,\n",
    "    data_cache_dir=DATA_CACHE_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-4\n",
    "optimizer = get_optimizer(\n",
    "    model=model,\n",
    "    device_type='cuda',\n",
    "    learning_rate=learning_rate,  # max learning rate\n",
    "    weight_decay = 1e-1,\n",
    "    beta1 = 0.9,\n",
    "    beta2 = 0.95,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 25000\n",
    "eval_iters = 100\n",
    "best_val_loss = 1e9\n",
    "grad_clip = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = 'Write a story. In the story, try to use the verb \"eat\", the noun \"clock\" and the adjective \"clever\". The story has the following features: the story should contain at least one dialogue. Possible story:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_num = 0\n",
    "\n",
    "train_batch_iter = iter_batches(split='train')\n",
    "X, Y = next(train_batch_iter)\n",
    "\n",
    "while True:\n",
    "    lr = get_lr(iter_num, max_iters=max_iters) \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "    if iter_num % eval_iters == 0 :\n",
    "        losses = estimate_loss(\n",
    "            model=model,\n",
    "            iter_batches=iter_batches,\n",
    "            eval_iters=eval_iters,\n",
    "            ctx=ctx\n",
    "        )\n",
    "        print(f\"step {iter_num}: lr {lr}, train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if losses[\"val\"] < best_val_loss:\n",
    "            best_val_loss = losses[\"val\"]\n",
    "            if iter_num > 0:\n",
    "                save_checkpoint(\n",
    "                    model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    model_args=model_args,\n",
    "                    iter_num=iter_num,\n",
    "                    out_dir=out_dir\n",
    "                )\n",
    "                _, paragraph = generate_paragraph(\n",
    "                    model, \n",
    "                    prompt=eval_prompt,\n",
    "                    tokenizer=tokenizer,\n",
    "                    device='cuda:0'\n",
    "                )\n",
    "                print(paragraph)\n",
    "\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        with ctx:\n",
    "            logits = model(X)\n",
    "            loss = compute_loss(logits, Y)\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        X, Y = next(train_batch_iter)\n",
    "        scaler.scale(loss).backward()\n",
    "     \n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "   \n",
    "    iter_num += 1\n",
    "    if iter_num > max_iters:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llama2karpathy]",
   "language": "python",
   "name": "conda-env-llama2karpathy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
